{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "svm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO+skeJaDuBxjT/LBRmS41g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauravrox/sentiment-analysis/blob/main/svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSeiAlCJFdPU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfyIgILLgHDS",
        "outputId": "27410e1a-6710-40d0-a01c-fb453a782239"
      },
      "source": [
        "import pandas as pd \n",
        "import tkinter as tk\n",
        "from tkinter import simpledialog\n",
        "import tweepy\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "!pip install emoji\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
        "import pickle\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib\n",
        "# print(sklearn.__version__)\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 14.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 19.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 81kB 6.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 102kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 112kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 122kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.1MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "XsWfE4DYgZBG",
        "outputId": "adfd6594-bdf1-42cf-e9f4-8294789219c5"
      },
      "source": [
        "# Local directory\n",
        "Reviewdata = pd.read_csv('/content/gdrive/MyDrive/thesis/train.csv', encoding='latin-1')\n",
        "Reviewdata = Reviewdata.dropna()\n",
        "count = Reviewdata.isnull().sum().sort_values(ascending=False)\n",
        "percentage = ((Reviewdata.isnull().sum()/len(Reviewdata)*100)).sort_values(ascending=False)\n",
        "missing_data = pd.concat([count, percentage], axis=1,\n",
        "keys=['Count','Percentage'])\n",
        "# print(Reviewdata['sentiment'].value_counts())\n",
        "Reviewdata['sentiment'].value_counts().sort_values().plot(kind = 'barh')\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "g5-3UcsvgcFm",
        "outputId": "1219109b-d3cb-45ab-e3dd-e95efc7af954"
      },
      "source": [
        "# Apply first level cleaning\n",
        "import re\n",
        "import string\n",
        "\n",
        "#This function converts to lower-case, removes square bracket, removes numbers and punctuation\n",
        "def text_clean(text):\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned = lambda x: text_clean(x)\n",
        "Reviewdata['cleaned_description'] = pd.DataFrame(Reviewdata.selected_text.apply(cleaned))\n",
        "Reviewdata.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>cleaned_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "      <td>negative</td>\n",
              "      <td>switchfoot   Awww thats a bummer  You shoulda ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>is upset that he cant update his Facebook by t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Kenichan I dived many times for the ball Manag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "      <td>negative</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "      <td>negative</td>\n",
              "      <td>nationwideclass no its not behaving at all im ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "      <td>negative</td>\n",
              "      <td>Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Need a hug</td>\n",
              "      <td>negative</td>\n",
              "      <td>Need a hug</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
              "      <td>negative</td>\n",
              "      <td>LOLTrish hey  long time no see Yes Rains a bit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@Tatiana_K nope they didn't have it</td>\n",
              "      <td>negative</td>\n",
              "      <td>TatianaK nope they didnt have it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@twittera que me muera ?</td>\n",
              "      <td>negative</td>\n",
              "      <td>twittera que me muera</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       selected_text  ...                                cleaned_description\n",
              "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  ...  switchfoot   Awww thats a bummer  You shoulda ...\n",
              "1  is upset that he can't update his Facebook by ...  ...  is upset that he cant update his Facebook by t...\n",
              "2  @Kenichan I dived many times for the ball. Man...  ...  Kenichan I dived many times for the ball Manag...\n",
              "3    my whole body feels itchy and like its on fire   ...    my whole body feels itchy and like its on fire \n",
              "4  @nationwideclass no, it's not behaving at all....  ...  nationwideclass no its not behaving at all im ...\n",
              "5                      @Kwesidei not the whole crew   ...                       Kwesidei not the whole crew \n",
              "6                                        Need a hug   ...                                        Need a hug \n",
              "7  @LOLTrish hey  long time no see! Yes.. Rains a...  ...  LOLTrish hey  long time no see Yes Rains a bit...\n",
              "8               @Tatiana_K nope they didn't have it   ...                  TatianaK nope they didnt have it \n",
              "9                          @twittera que me muera ?   ...                            twittera que me muera  \n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2AWfMt9geaV"
      },
      "source": [
        "def extract_emojis(s):\n",
        "\ttweet = emoji.demojize(s)\n",
        "\ttweet = tweet.replace(\":\",\" \")\n",
        "\ttweet = ' '.join(tweet.split())\n",
        "\treturn tweet\n",
        "\n",
        "# Let's take a look at the updated text\n",
        "Reviewdata['cleaned_description_new'] = pd.DataFrame(Reviewdata['cleaned_description'].apply(cleaned))\n",
        "Reviewdata['sentiment'] = LabelEncoder().fit_transform(Reviewdata['sentiment'])\n",
        "# Reviewdata['sentiment'].plot(kind=\"bar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akLPhN0hgheN"
      },
      "source": [
        "Reviewdata['sentiment'] = LabelEncoder().fit_transform(Reviewdata['sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf4inUHdgi9t",
        "outputId": "e4dff241-718e-46b1-eded-38ccfdbc3d0b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "Independent_var = Reviewdata.cleaned_description_new\n",
        "Dependent_var = Reviewdata.sentiment\n",
        "\n",
        "IV_train, IV_test, DV_train, DV_test = train_test_split(Independent_var, Dependent_var, test_size = 0.25, random_state = 225)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tvec = TfidfVectorizer()\n",
        "#Create a svm Classifier\n",
        "clf_svm = svm.SVC(kernel='linear') # Linear Kernel\n",
        "svm_model = Pipeline([('vectorizer',tvec),('classifier',clf_svm)])\n",
        "\n",
        "#Train the model using the training sets\n",
        "svm_model.fit(IV_train, DV_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='linear', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A3IxpzXSuVb",
        "outputId": "5505c82b-91ff-405d-ce48-12b4fe45780f"
      },
      "source": [
        "lin_pred = svm_model.predict(IV_test)\n",
        "lin_accuracy = accuracy_score(lin_pred,DV_test)\n",
        "lin_f1 = f1_score(lin_pred,DV_test, average='weighted')\n",
        "print('Accuracy (linnomial Kernel): ', \"%.2f\" % (lin_accuracy*100))\n",
        "print('F1 (linnomial Kernel): ', \"%.2f\" % (lin_f1*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy (linnomial Kernel):  68.00\n",
            "F1 (linnomial Kernel):  68.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1dzIdvygqC2",
        "outputId": "b4572221-c9d9-4302-d6da-5291215413d2"
      },
      "source": [
        "le = LabelEncoder()\n",
        "# df.region = le.fit_transform(df.region)\n",
        "\n",
        "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1)\n",
        "rbf_svm_model = Pipeline([('vectorizer',tvec),('classifier',rbf)])\n",
        "\n",
        "#Train the model using the training sets\n",
        "rbf_svm_model.fit(IV_train, DV_train) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma=0.5, kernel='rbf', max_iter=-1, probability=False,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClE1u_dDLdfw",
        "outputId": "ec154a30-67f2-48b7-b17a-4b75bf3cb4d1"
      },
      "source": [
        "rbf_svm_model_pred = rbf_svm_model.predict(IV_test)\n",
        "# rbf_pred = rbf.predict(IV_test)\n",
        "\n",
        "rbf_svm_model_accuracy = accuracy_score(rbf_svm_model_pred,DV_test)\n",
        "rbf_svm_model_f1 = f1_score(rbf_svm_model_pred,DV_test, average='weighted')\n",
        "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_svm_model_accuracy*100))\n",
        "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_svm_model_f1*100))\n",
        "# In the same way, the accuracy and f1 scores for SVM with RBF kernel:"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy (RBF Kernel):  62.58\n",
            "F1 (RBF Kernel):  63.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-Vq3mcdhCg1",
        "outputId": "222f2256-bb58-4ee0-c27a-10310cad97ad"
      },
      "source": [
        "# filename = 'svm_model.sav'\n",
        "# pickle.dump(svm_model, open(filename, 'wb'))\n",
        "print(\"SVM Confusion Matrix: \", confusion_matrix(rbf_svm_model_pred, DV_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Confusion Matrix:  [[6293 2535 2000]\n",
            " [  68 3090   64]\n",
            " [1979 2704 6255]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFbcjHo_gusd"
      },
      "source": [
        "poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo')\n",
        "poly_svm_model = Pipeline([('vectorizer',tvec),('classifier',poly)])\n",
        "\n",
        "# #Train the model using the training sets\n",
        "# poly_svm_model.fit(IV_train, DV_train)\n",
        "poly = poly_svm_model.fit(IV_train, DV_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwWWbeYlg6W9",
        "outputId": "23dbba09-5d11-4ff5-9cd4-c6566e3c379c"
      },
      "source": [
        "poly_pred = poly.predict(IV_test)\n",
        "# rbf_pred = rbf.predict(IV_test)\n",
        "\n",
        "poly_accuracy = accuracy_score(poly_pred,DV_test)\n",
        "poly_f1 = f1_score(poly_pred,DV_test, average='weighted')\n",
        "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
        "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n",
        "# In the same way, the accuracy and f1 scores for SVM with RBF kernel:"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy (Polynomial Kernel):  74.83\n",
            "F1 (Polynomial Kernel):  74.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alHy31syg8fd",
        "outputId": "11ac0b33-72da-4c00-9f78-06df368a01a5"
      },
      "source": [
        "print(\"SVM Confusion Matrix: \", confusion_matrix(poly_pred, DV_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Confusion Matrix:  [[6219  933 1510]\n",
            " [ 308 6057  386]\n",
            " [1813 1339 6423]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqiuhJZWg-Xd",
        "outputId": "43159710-361e-4959-ade5-f45f616438b7"
      },
      "source": [
        "print(\"SVM Report : \", \n",
        "classification_report(poly_pred, DV_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.72      0.73      8662\n",
            "           1       0.73      0.90      0.80      6751\n",
            "           2       0.77      0.67      0.72      9575\n",
            "\n",
            "    accuracy                           0.75     24988\n",
            "   macro avg       0.75      0.76      0.75     24988\n",
            "weighted avg       0.75      0.75      0.75     24988\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDdQ0CPwhEYM"
      },
      "source": [
        "consumerKey = 'Y76b838VJixCT9coY5aVaFWRv'\n",
        "consumerSecret = 'MWQiIs4rzY8zKySusGf6hPiLU8e3vAbJNV02lKi3FLhdfirUmT'\n",
        "accessToken = '1266387212150566913-HS7RCIKe86EsZFmMVMjeOF3SWlvCY5'\n",
        "accessTokenSecret = 'AnngLumAWhaKXw1avMXR8eWu1GUllVMrML1zQKy1poLBP'\n",
        "\n",
        "# Create the authentication object\n",
        "authenticate = tweepy.OAuthHandler(consumerKey, consumerSecret) \n",
        "    \n",
        "# Set the access token and access token secret\n",
        "authenticate.set_access_token(accessToken, accessTokenSecret) \n",
        "    \n",
        "# Creating the API object while passing in auth information\n",
        "api = tweepy.API(authenticate, wait_on_rate_limit = True)\n",
        "\n",
        "# Extract 100 tweets from the twitter user\n",
        "# ROOT = tk.Tk()\n",
        "\n",
        "# ROOT.withdraw()\n",
        "# the input dialog\n",
        "# USER_INP = simpledialog.askstring(title=\"Fetch Tweets\", prompt=\"Write your Twitter Handle.\")\n",
        "# posts = api.user_timeline(screen_name=USER_INP, count = 100, lang =\"en\", tweet_mode=\"extended\")\n",
        "user_data = '%23'+'biden'\n",
        "###########################################################################\n",
        "# Create a dataframe with a column called Tweets\n",
        "# :\n",
        "# Cache the stop words for speed \n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "posts = list()\n",
        "for tweet in tweepy.Cursor(api.search, q=user_data, lang='en').items(500):\n",
        "\tposts.append(tweet.text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZwImA8IUaeP"
      },
      "source": [
        "lemmatiser = WordNetLemmatizer()\n",
        "list_posts = list()\n",
        "for p in posts:\n",
        "\ttemp = re.sub(' +', ' ', p).lower()\n",
        "\ttemp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n",
        "\ttemp = extract_emojis(temp)\n",
        "\ttemp = text_clean(temp)\n",
        "\tlist_posts.append(temp)\n",
        "# print(list_posts)\n",
        "df = pd.DataFrame([list_post for list_post in list_posts], columns=['Tweets'])\n",
        "\n",
        "# Show the first 5 rows of data\n",
        "df.head()\n",
        "\n",
        "# Clean the tweets\n",
        "df['Tweets'] = df['Tweets'].apply(text_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPCye8r5hGVl"
      },
      "source": [
        "#Predict the response for test dataset\n",
        "# Create a function to get the polarity\n",
        "def svm_getPolarity(text):\n",
        "\texample = [text]\n",
        "\tresult = poly.predict(example)\n",
        "\tif result==1:\n",
        "\t\tresult = 'neutral'\n",
        "\tif result==2:\n",
        "\t\tresult = 'positive'\n",
        "\tif result==0:\n",
        "\t\tresult = 'negative'\n",
        "\treturn result\n",
        "\n",
        "# y_pred = clf_svm.predict(IV_test)\n",
        "\n",
        "df['SVM_Polarity'] = df['Tweets'].apply(svm_getPolarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1aSWaIAIhKd1",
        "outputId": "069aad42-9072-431d-8216-47c7cd17b33a"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweets</th>\n",
              "      <th>SVM_Polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bidens plan narrow racial wealth gap https  ax...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rt marionmckeone “my fellow americans riot mas...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>rt  people stop wasting lot time trying improv...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>rt  UnitedStates an emotional joe biden marked...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>billionaire businesswoman u ex president donal...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>rt arunpudur trump said biden fuel price skyro...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>disaster declaration issued texas abbott secur...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>pray write stupid stuff online trump biden usa...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>rt peaceupatl personfacepalmingmediumskintone ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>rt fsbohighlands biden tap harris lead effort ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Tweets SVM_Polarity\n",
              "0    bidens plan narrow racial wealth gap https  ax...     positive\n",
              "1    rt marionmckeone “my fellow americans riot mas...     positive\n",
              "2    rt  people stop wasting lot time trying improv...     positive\n",
              "3    rt  UnitedStates an emotional joe biden marked...     positive\n",
              "4    billionaire businesswoman u ex president donal...     positive\n",
              "..                                                 ...          ...\n",
              "495  rt arunpudur trump said biden fuel price skyro...     positive\n",
              "496  disaster declaration issued texas abbott secur...     positive\n",
              "497  pray write stupid stuff online trump biden usa...     positive\n",
              "498  rt peaceupatl personfacepalmingmediumskintone ...     positive\n",
              "499  rt fsbohighlands biden tap harris lead effort ...     positive\n",
              "\n",
              "[500 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}