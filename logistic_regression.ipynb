{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic-regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJPZs+nz/u7w6eo0MOg6wS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauravrox/sentiment-analysis/blob/main/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr1IpGFobfso",
        "outputId": "396db991-3b59-4bf2-ee40-649b98108bae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HJkdxv5b6An",
        "outputId": "6a6d5939-ef6a-4925-f8eb-2565a6ab0a18"
      },
      "source": [
        "import pandas as pd \n",
        "import tkinter as tk\n",
        "import tweepy\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "!pip install emoji\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
        "import pickle\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "import matplotlib\n",
        "# print(sklearn.__version__)\n",
        "matplotlib.use('Agg')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiEga0U_cAOF"
      },
      "source": [
        "# Local directory\n",
        "Reviewdata = pd.read_csv('/content/drive/MyDrive/thesis/train.csv', encoding='latin-1')\n",
        "Reviewdata = Reviewdata.dropna()\n",
        "count = Reviewdata.isnull().sum().sort_values(ascending=False)\n",
        "percentage = ((Reviewdata.isnull().sum()/len(Reviewdata)*100)).sort_values(ascending=False)\n",
        "missing_data = pd.concat([count, percentage], axis=1,\n",
        "keys=['Count','Percentage'])\n",
        "Reviewdata['sentiment'].value_counts().sort_values().plot(kind = 'barh')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "yV3POLoNcHVV",
        "outputId": "73d92650-99f5-4000-bb7a-a2402c2232b5"
      },
      "source": [
        "# Apply first level cleaning\n",
        "import re\n",
        "import string\n",
        "\n",
        "#This function converts to lower-case, removes square bracket, removes numbers and punctuation\n",
        "def text_clean(text):\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned = lambda x: text_clean(x)\n",
        "Reviewdata['cleaned_description'] = pd.DataFrame(Reviewdata.selected_text.apply(cleaned))\n",
        "Reviewdata.head(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>cleaned_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "      <td>negative</td>\n",
              "      <td>switchfoot   Awww thats a bummer  You shoulda ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>is upset that he cant update his Facebook by t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Kenichan I dived many times for the ball Manag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "      <td>negative</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "      <td>negative</td>\n",
              "      <td>nationwideclass no its not behaving at all im ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "      <td>negative</td>\n",
              "      <td>Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Need a hug</td>\n",
              "      <td>negative</td>\n",
              "      <td>Need a hug</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
              "      <td>negative</td>\n",
              "      <td>LOLTrish hey  long time no see Yes Rains a bit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@Tatiana_K nope they didn't have it</td>\n",
              "      <td>negative</td>\n",
              "      <td>TatianaK nope they didnt have it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@twittera que me muera ?</td>\n",
              "      <td>negative</td>\n",
              "      <td>twittera que me muera</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       selected_text  ...                                cleaned_description\n",
              "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  ...  switchfoot   Awww thats a bummer  You shoulda ...\n",
              "1  is upset that he can't update his Facebook by ...  ...  is upset that he cant update his Facebook by t...\n",
              "2  @Kenichan I dived many times for the ball. Man...  ...  Kenichan I dived many times for the ball Manag...\n",
              "3    my whole body feels itchy and like its on fire   ...    my whole body feels itchy and like its on fire \n",
              "4  @nationwideclass no, it's not behaving at all....  ...  nationwideclass no its not behaving at all im ...\n",
              "5                      @Kwesidei not the whole crew   ...                       Kwesidei not the whole crew \n",
              "6                                        Need a hug   ...                                        Need a hug \n",
              "7  @LOLTrish hey  long time no see! Yes.. Rains a...  ...  LOLTrish hey  long time no see Yes Rains a bit...\n",
              "8               @Tatiana_K nope they didn't have it   ...                  TatianaK nope they didnt have it \n",
              "9                          @twittera que me muera ?   ...                            twittera que me muera  \n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnatuTufcK5h"
      },
      "source": [
        "def extract_emojis(s):\n",
        "\ttweet = emoji.demojize(s)\n",
        "\ttweet = tweet.replace(\":\",\" \")\n",
        "\ttweet = ' '.join(tweet.split())\n",
        "\treturn tweet\n",
        "\n",
        "# Let's take a look at the updated text\n",
        "Reviewdata['cleaned_description_new'] = pd.DataFrame(Reviewdata['cleaned_description'].apply(cleaned))\n",
        "Reviewdata['sentiment'] = LabelEncoder().fit_transform(Reviewdata['sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9I6zQl-cPGW"
      },
      "source": [
        "Reviewdata['sentiment'] = LabelEncoder().fit_transform(Reviewdata['sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skUMnrwfcSAy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "Independent_var = Reviewdata.cleaned_description_new\n",
        "Dependent_var = Reviewdata.sentiment\n",
        "\n",
        "IV_train, IV_test, DV_train, DV_test = train_test_split(Independent_var, Dependent_var, test_size = 0.25, random_state = 225)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "tvec = TfidfVectorizer()\n",
        "\n",
        "clf2 = LogisticRegression(solver = \"liblinear\")\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "IV_train, IV_test, DV_train, DV_test = model_selection.train_test_split(Independent_var, Dependent_var, test_size=0.25, random_state=109)\n",
        "\n",
        "model = Pipeline([('vectorizer',tvec),('classifier',clf2)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdwuxPgscWD-",
        "outputId": "03275af6-87f2-4fbb-abe6-57f37a2a203b"
      },
      "source": [
        "model.fit(IV_train, DV_train)\n",
        "predictions = model.predict(IV_test)\n",
        "print(\"Logistic Confusion Matrix: \", confusion_matrix(predictions, DV_test)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Confusion Matrix:  [[6078 1752 1452]\n",
            " [ 751 4845  881]\n",
            " [1505 1709 6015]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DedjLeyXccwW",
        "outputId": "0468e770-8979-4da6-e259-a27bb84170b7"
      },
      "source": [
        "print (\"Logistic Accuracy : \", \n",
        "accuracy_score(predictions,DV_test)*100) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Accuracy :  67.78453657755723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywczTDq7cfpy",
        "outputId": "5d87a82e-cd45-4bcc-d974-82abc10df53f"
      },
      "source": [
        "print(\"Logistic Report : \", \n",
        "classification_report(predictions, DV_test)) \n",
        "\n",
        "kfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n",
        "scoring = 'accuracy'\n",
        "results = model_selection.cross_val_score(model, Independent_var, Dependent_var, cv=kfold, scoring=scoring)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.65      0.69      9282\n",
            "           1       0.58      0.75      0.66      6477\n",
            "           2       0.72      0.65      0.68      9229\n",
            "\n",
            "    accuracy                           0.68     24988\n",
            "   macro avg       0.68      0.68      0.68     24988\n",
            "weighted avg       0.69      0.68      0.68     24988\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3V-O18ycjC5",
        "outputId": "4b4ca8a7-bb4e-498e-b96a-bbff7ba4fc68"
      },
      "source": [
        "print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.678 (0.003)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQD_dD_ycplJ"
      },
      "source": [
        "consumerKey = 'Y76b838VJixCT9coY5aVaFWRv'\n",
        "consumerSecret = 'MWQiIs4rzY8zKySusGf6hPiLU8e3vAbJNV02lKi3FLhdfirUmT'\n",
        "accessToken = '1266387212150566913-HS7RCIKe86EsZFmMVMjeOF3SWlvCY5'\n",
        "accessTokenSecret = 'AnngLumAWhaKXw1avMXR8eWu1GUllVMrML1zQKy1poLBP'\n",
        "\n",
        "# Create the authentication object\n",
        "authenticate = tweepy.OAuthHandler(consumerKey, consumerSecret) \n",
        "    \n",
        "# Set the access token and access token secret\n",
        "authenticate.set_access_token(accessToken, accessTokenSecret) \n",
        "    \n",
        "# Creating the API object while passing in auth information\n",
        "api = tweepy.API(authenticate, wait_on_rate_limit = True)\n",
        "\n",
        "user_data = '%23'+'biden'\n",
        "\n",
        "# Cache the stop words for speed \n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "posts = list()\n",
        "for tweet in tweepy.Cursor(api.search, q=user_data, lang='en').items(500):\n",
        "\tposts.append(tweet.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpie_m8rcvcE"
      },
      "source": [
        "lemmatiser = WordNetLemmatizer()\n",
        "list_posts = list()\n",
        "for p in posts:\n",
        "\ttemp = re.sub(' +', ' ', p).lower()\n",
        "\ttemp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n",
        "\ttemp = extract_emojis(temp)\n",
        "\ttemp = text_clean(temp)\n",
        "\tlist_posts.append(temp)\n",
        "# print(list_posts)\n",
        "df = pd.DataFrame([list_post for list_post in list_posts], columns=['Tweets'])\n",
        "\n",
        "# Show the first 5 rows of data\n",
        "df.head()\n",
        "\n",
        "# Clean the tweets\n",
        "df['Tweets'] = df['Tweets'].apply(text_clean)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QButYScrcwXS"
      },
      "source": [
        "# Create a function to get the polarity\n",
        "def getPolarity(text):\n",
        "\texample = [text]\n",
        "\tresult = model.predict(example)\n",
        "\tif result==1:\n",
        "\t\tresult = 'neutral'\n",
        "\tif result==2:\n",
        "\t\tresult = 'positive'\n",
        "\tif result==0:\n",
        "\t\tresult = 'negative'\n",
        "\treturn result\n",
        "\n",
        "# # Create two new columns 'Subjectivity' & 'Polarity'\n",
        "df['Polarity'] = df['Tweets'].apply(getPolarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldeqti0MiMah",
        "outputId": "1a766d89-4538-4232-8c12-3313391b9dbc"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Tweets  Polarity\n",
            "0    rt guinnchastity breaking news potus stand pre...   neutral\n",
            "1    finally leader speaking there structural racis...   neutral\n",
            "2    ryanlcooper alysonmetzger battle fought deaf e...   neutral\n",
            "3    fuckarianetabatabai biden remove robertmolly i...  positive\n",
            "4    rt ypfdjtoronto national council eritrean amer...  positive\n",
            "..                                                 ...       ...\n",
            "495  rt hwfloyd im concerned biden administration w...  positive\n",
            "496  daily reminder trump donated entire four year ...  positive\n",
            "497  potus soon weak as biden became president targ...   neutral\n",
            "498  Russia weekend novaya zemlya central nuclear t...   neutral\n",
            "499  rt hwfloyd im concerned biden administration w...  positive\n",
            "\n",
            "[500 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}